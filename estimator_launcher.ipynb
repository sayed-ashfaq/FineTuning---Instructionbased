{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3e02ed-6dc3-45eb-8c31-774210673d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b0bc7c-077b-4de8-9237-fa59ffa86d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d7cce1-b171-4bf5-aadc-a3d4203ef4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::445792334809:role/SageMakerLLMRole'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be16c83f-f3e4-4280-a61e-c52f73ebc466",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters= {\n",
    "    \"model_id\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
    "    'epochs': 2, \n",
    "    'per_device_train_batch_size': 2,\n",
    "    'lr': 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32d3ee2c-2938-4697-8715-2d6b3a2c500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = HuggingFace(\n",
    "    entry_point= \"train.py\",\n",
    "    source_dir = './scripts',\n",
    "    role= role,\n",
    "    transformers_version= '4.36',\n",
    "    pytorch_version= '2.1',\n",
    "    py_version='py310',\n",
    "    instance_type= 'ml.g5.xlarge',\n",
    "    instance_count= 1,\n",
    "    output_path=\"s3://llm-model-artifacts-sayed/models/\",\n",
    "    hyperparameters= hyperparameters\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8359ba7e-84ac-48e7-b6d1-7496f5f2b28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2025-11-27-12-31-33-882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 12:31:34 Starting - Starting the training job...\n",
      "2025-11-27 12:31:58 Pending - Training job waiting for capacity...\n",
      "2025-11-27 12:32:35 Downloading - Downloading input data...\n",
      "2025-11-27 12:33:00 Downloading - Downloading the training image........................\n",
      "2025-11-27 12:36:53 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:03,093 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:03,111 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:03,121 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:03,128 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:05,157 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:05,186 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:05,214 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:05,225 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2,\n",
      "        \"lr\": 2e-05,\n",
      "        \"model_id\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2025-11-27-12-31-33-882\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://llm-model-artifacts-sayed/huggingface-pytorch-training-2025-11-27-12-31-33-882/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2,\"lr\":2e-05,\"model_id\":\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\"per_device_train_batch_size\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://llm-model-artifacts-sayed/huggingface-pytorch-training-2025-11-27-12-31-33-882/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"lr\":2e-05,\"model_id\":\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2025-11-27-12-31-33-882\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://llm-model-artifacts-sayed/huggingface-pytorch-training-2025-11-27-12-31-33-882/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\",\"--lr\",\"2e-05\",\"--model_id\",\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\"--per_device_train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LR=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --epochs 2 --lr 2e-05 --model_id TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T --per_device_train_batch_size 2\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:05,225 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-11-27 12:37:05,225 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===== Loading Dataset =====\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3000 examples [00:00, 278463.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/3000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 3000/3000 [00:00<00:00, 29698.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 3000/3000 [00:00<00:00, 29397.82 examples/s]\u001b[0m\n",
      "\u001b[34m===== Loading Tokenizer =====\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/3000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 1000/3000 [00:00<00:00, 4815.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 2000/3000 [00:00<00:00, 5020.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 3000/3000 [00:00<00:00, 5038.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 3000/3000 [00:00<00:00, 4976.34 examples/s]\u001b[0m\n",
      "\u001b[34m===== Loading Base Model in 4bit =====\u001b[0m\n",
      "\u001b[34m===== Applying QLoRA =====\u001b[0m\n",
      "\u001b[34m===== Setting Training Params =====\u001b[0m\n",
      "\u001b[34m===== Starting Training =====\u001b[0m\n",
      "\u001b[34m0%|          | 0/374 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 1/374 [00:04<30:48,  4.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/374 [00:07<20:54,  3.37s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/374 [00:09<17:45,  2.87s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/374 [00:11<15:52,  2.57s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/374 [00:13<14:37,  2.38s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/374 [00:15<13:52,  2.26s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/374 [00:17<13:22,  2.19s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/374 [00:19<13:02,  2.14s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9/374 [00:21<12:47,  2.10s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/374 [00:23<12:36,  2.08s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/374 [00:25<12:29,  2.06s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/374 [00:27<12:23,  2.05s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13/374 [00:29<12:18,  2.05s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 14/374 [00:31<12:14,  2.04s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/374 [00:33<12:10,  2.04s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 16/374 [00:35<12:07,  2.03s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/374 [00:37<12:05,  2.03s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 18/374 [00:40<12:02,  2.03s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 19/374 [00:42<12:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 20/374 [00:44<11:57,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 11.9378, 'learning_rate': 1.9144385026737972e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m5%|▌         | 20/374 [00:44<11:57,  2.03s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/374 [00:46<11:55,  2.03s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 22/374 [00:48<11:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 23/374 [00:50<11:51,  2.03s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 24/374 [00:52<11:49,  2.03s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/374 [00:54<11:47,  2.03s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 26/374 [00:56<11:45,  2.03s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 27/374 [00:58<11:43,  2.03s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 28/374 [01:00<11:41,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/374 [01:02<11:39,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 30/374 [01:04<11:37,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 31/374 [01:06<11:35,  2.03s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 32/374 [01:08<11:32,  2.03s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/374 [01:10<11:31,  2.03s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 34/374 [01:12<11:29,  2.03s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 35/374 [01:14<11:27,  2.03s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 36/374 [01:16<11:25,  2.03s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 37/374 [01:18<11:23,  2.03s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 38/374 [01:20<11:20,  2.03s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 39/374 [01:22<11:18,  2.03s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 40/374 [01:24<11:16,  2.03s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 40/374 [01:24<11:16,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 10.8045, 'learning_rate': 1.8128342245989307e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m11%|█         | 41/374 [01:26<11:14,  2.03s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 42/374 [01:28<11:12,  2.03s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 43/374 [01:30<11:10,  2.03s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 44/374 [01:32<11:08,  2.03s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 45/374 [01:34<11:06,  2.03s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 46/374 [01:36<11:04,  2.03s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/374 [01:38<11:02,  2.03s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 48/374 [01:40<11:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 49/374 [01:42<10:58,  2.02s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 50/374 [01:44<10:56,  2.02s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 51/374 [01:46<10:54,  2.02s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 52/374 [01:48<10:51,  2.02s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 53/374 [01:50<10:50,  2.03s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 54/374 [01:52<10:48,  2.03s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 55/374 [01:54<10:45,  2.02s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 56/374 [01:56<10:43,  2.02s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 57/374 [01:59<10:41,  2.02s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 58/374 [02:01<10:39,  2.02s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 59/374 [02:03<10:37,  2.02s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 60/374 [02:05<10:35,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 8.0255, 'learning_rate': 1.7058823529411767e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 60/374 [02:05<10:35,  2.02s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 61/374 [02:07<10:33,  2.02s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 62/374 [02:09<10:31,  2.02s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 63/374 [02:11<10:29,  2.02s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 64/374 [02:13<10:28,  2.03s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 65/374 [02:15<10:25,  2.03s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 66/374 [02:17<10:23,  2.03s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 67/374 [02:19<10:21,  2.03s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 68/374 [02:21<10:19,  2.02s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 69/374 [02:23<10:17,  2.02s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 70/374 [02:25<10:15,  2.02s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 71/374 [02:27<10:13,  2.02s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 72/374 [02:29<10:11,  2.02s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 73/374 [02:31<10:09,  2.03s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 74/374 [02:33<10:07,  2.02s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 75/374 [02:35<10:05,  2.03s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 76/374 [02:37<10:03,  2.03s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 77/374 [02:39<10:01,  2.03s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 78/374 [02:41<09:59,  2.03s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 79/374 [02:43<09:57,  2.03s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 80/374 [02:45<09:55,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.4147, 'learning_rate': 1.5989304812834226e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 80/374 [02:45<09:55,  2.02s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 81/374 [02:47<09:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 82/374 [02:49<09:51,  2.02s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 83/374 [02:51<09:49,  2.02s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 84/374 [02:53<09:47,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 85/374 [02:55<09:45,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 86/374 [02:57<09:43,  2.03s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 87/374 [02:59<09:41,  2.03s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 88/374 [03:01<09:39,  2.03s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 89/374 [03:03<09:37,  2.02s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 90/374 [03:05<09:34,  2.02s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 91/374 [03:07<09:32,  2.02s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 92/374 [03:09<09:30,  2.02s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 93/374 [03:11<09:29,  2.03s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 94/374 [03:13<09:27,  2.03s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 95/374 [03:15<09:25,  2.03s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 96/374 [03:18<09:23,  2.03s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 97/374 [03:20<09:21,  2.03s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 98/374 [03:22<09:18,  2.03s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 99/374 [03:24<09:16,  2.02s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 100/374 [03:26<09:14,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0795, 'learning_rate': 1.4919786096256686e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 100/374 [03:26<09:14,  2.03s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 101/374 [03:28<09:12,  2.03s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 102/374 [03:30<09:10,  2.02s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 103/374 [03:32<09:08,  2.02s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 104/374 [03:34<09:06,  2.02s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 105/374 [03:36<09:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 106/374 [03:38<09:02,  2.02s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 107/374 [03:40<09:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 108/374 [03:42<08:58,  2.02s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 109/374 [03:44<08:56,  2.02s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 110/374 [03:46<08:54,  2.02s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 111/374 [03:48<08:52,  2.02s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 112/374 [03:50<08:50,  2.02s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 113/374 [03:52<08:48,  2.02s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 114/374 [03:54<08:46,  2.02s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 115/374 [03:56<08:44,  2.02s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 116/374 [03:58<08:42,  2.02s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 117/374 [04:00<08:40,  2.02s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 118/374 [04:02<08:38,  2.02s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 119/374 [04:04<08:36,  2.02s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 120/374 [04:06<08:34,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8043, 'learning_rate': 1.3850267379679146e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 120/374 [04:06<08:34,  2.02s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 121/374 [04:08<08:32,  2.02s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 122/374 [04:10<08:30,  2.02s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 123/374 [04:12<08:28,  2.02s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 124/374 [04:14<08:26,  2.02s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 125/374 [04:16<08:24,  2.02s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 126/374 [04:18<08:22,  2.02s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 127/374 [04:20<08:20,  2.02s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 128/374 [04:22<08:18,  2.02s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 129/374 [04:24<08:15,  2.02s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 130/374 [04:26<08:14,  2.02s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 131/374 [04:28<08:12,  2.02s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 132/374 [04:30<08:09,  2.02s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 133/374 [04:32<08:07,  2.02s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 134/374 [04:34<08:05,  2.02s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 135/374 [04:36<08:03,  2.02s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 136/374 [04:38<08:01,  2.02s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 137/374 [04:41<07:59,  2.02s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 138/374 [04:43<07:57,  2.02s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 139/374 [04:45<07:55,  2.02s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 140/374 [04:47<07:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3379, 'learning_rate': 1.2780748663101605e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 140/374 [04:47<07:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 141/374 [04:49<07:51,  2.02s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 142/374 [04:51<07:49,  2.02s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 143/374 [04:53<07:47,  2.02s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 144/374 [04:55<07:45,  2.03s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 145/374 [04:57<07:43,  2.02s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 146/374 [04:59<07:41,  2.03s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 147/374 [05:01<07:39,  2.03s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 148/374 [05:03<07:37,  2.03s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 149/374 [05:05<07:35,  2.03s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 150/374 [05:07<07:33,  2.03s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 151/374 [05:09<07:31,  2.03s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 152/374 [05:11<07:29,  2.03s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 153/374 [05:13<07:27,  2.03s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 154/374 [05:15<07:25,  2.03s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 155/374 [05:17<07:23,  2.03s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 156/374 [05:19<07:21,  2.03s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 157/374 [05:21<07:19,  2.03s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 158/374 [05:23<07:17,  2.03s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 159/374 [05:25<07:15,  2.03s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 160/374 [05:27<07:13,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2458, 'learning_rate': 1.1711229946524065e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 160/374 [05:27<07:13,  2.03s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 161/374 [05:29<07:11,  2.03s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 162/374 [05:31<07:09,  2.02s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 163/374 [05:33<07:07,  2.02s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 164/374 [05:35<07:05,  2.02s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 165/374 [05:37<07:03,  2.02s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 166/374 [05:39<07:01,  2.02s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 167/374 [05:41<06:59,  2.02s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 168/374 [05:43<06:57,  2.02s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 169/374 [05:45<06:55,  2.02s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 170/374 [05:47<06:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 171/374 [05:49<06:50,  2.02s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 172/374 [05:51<06:48,  2.02s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 173/374 [05:53<06:46,  2.02s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 174/374 [05:55<06:44,  2.02s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 175/374 [05:57<06:42,  2.02s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 176/374 [05:59<06:40,  2.02s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 177/374 [06:02<06:38,  2.02s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 178/374 [06:04<06:36,  2.02s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 179/374 [06:06<06:34,  2.02s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 180/374 [06:08<06:32,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2312, 'learning_rate': 1.0641711229946525e-05, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 180/374 [06:08<06:32,  2.02s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 181/374 [06:10<06:30,  2.02s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 182/374 [06:12<06:33,  2.05s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 183/374 [06:14<06:29,  2.04s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 184/374 [06:16<06:27,  2.04s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 185/374 [06:18<06:24,  2.03s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 186/374 [06:20<06:21,  2.03s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 187/374 [06:22<06:19,  2.03s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 188/374 [06:24<06:17,  2.03s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 189/374 [06:26<06:14,  2.03s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 190/374 [06:28<06:12,  2.03s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 191/374 [06:30<06:10,  2.03s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 192/374 [06:32<06:08,  2.03s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 193/374 [06:34<06:06,  2.02s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 194/374 [06:36<06:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 195/374 [06:38<06:02,  2.02s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 196/374 [06:40<06:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 197/374 [06:42<05:58,  2.02s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 198/374 [06:44<05:56,  2.02s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 199/374 [06:46<05:54,  2.02s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 200/374 [06:48<05:52,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2211, 'learning_rate': 9.572192513368986e-06, 'epoch': 1.07}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 200/374 [06:48<05:52,  2.02s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 201/374 [06:50<05:50,  2.02s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 202/374 [06:52<05:48,  2.02s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 203/374 [06:54<05:46,  2.02s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 204/374 [06:56<05:44,  2.02s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 205/374 [06:58<05:42,  2.02s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 206/374 [07:00<05:40,  2.02s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 207/374 [07:02<05:38,  2.02s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 208/374 [07:04<05:36,  2.02s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 209/374 [07:06<05:34,  2.02s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 210/374 [07:08<05:31,  2.02s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 211/374 [07:10<05:29,  2.02s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 212/374 [07:12<05:28,  2.03s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 213/374 [07:14<05:26,  2.03s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 214/374 [07:17<05:24,  2.03s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 215/374 [07:19<05:22,  2.03s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 216/374 [07:21<05:19,  2.02s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 217/374 [07:23<05:17,  2.02s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 218/374 [07:25<05:15,  2.02s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 219/374 [07:27<05:13,  2.02s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 220/374 [07:29<05:11,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2152, 'learning_rate': 8.502673796791444e-06, 'epoch': 1.17}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 220/374 [07:29<05:11,  2.02s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 221/374 [07:31<05:09,  2.02s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 222/374 [07:33<05:07,  2.02s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 223/374 [07:35<05:05,  2.02s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 224/374 [07:37<05:03,  2.02s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 225/374 [07:39<05:01,  2.02s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 226/374 [07:41<04:59,  2.02s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 227/374 [07:43<04:57,  2.02s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 228/374 [07:45<04:55,  2.02s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 229/374 [07:47<04:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 230/374 [07:49<04:51,  2.02s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 231/374 [07:51<04:49,  2.02s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 232/374 [07:53<04:47,  2.02s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 233/374 [07:55<04:45,  2.02s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 234/374 [07:57<04:43,  2.03s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 235/374 [07:59<04:41,  2.03s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 236/374 [08:01<04:39,  2.02s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 237/374 [08:03<04:37,  2.03s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 238/374 [08:05<04:35,  2.03s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 239/374 [08:07<04:33,  2.03s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 240/374 [08:09<04:31,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2102, 'learning_rate': 7.433155080213904e-06, 'epoch': 1.28}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 240/374 [08:09<04:31,  2.03s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 241/374 [08:11<04:29,  2.03s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 242/374 [08:13<04:27,  2.03s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 243/374 [08:15<04:25,  2.03s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 244/374 [08:17<04:23,  2.03s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 245/374 [08:19<04:21,  2.03s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 246/374 [08:21<04:19,  2.03s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 247/374 [08:23<04:17,  2.03s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 248/374 [08:25<04:15,  2.03s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 249/374 [08:27<04:13,  2.03s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 250/374 [08:29<04:11,  2.03s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 251/374 [08:31<04:09,  2.03s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 252/374 [08:33<04:07,  2.03s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 253/374 [08:35<04:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 254/374 [08:38<04:03,  2.03s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 255/374 [08:40<04:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 256/374 [08:42<03:58,  2.02s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 257/374 [08:44<03:56,  2.02s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 258/374 [08:46<03:54,  2.02s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 259/374 [08:48<03:52,  2.02s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 260/374 [08:50<03:50,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2059, 'learning_rate': 6.363636363636364e-06, 'epoch': 1.39}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 260/374 [08:50<03:50,  2.02s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 261/374 [08:52<03:48,  2.02s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 262/374 [08:54<03:46,  2.02s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 263/374 [08:56<03:44,  2.02s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 264/374 [08:58<03:42,  2.02s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 265/374 [09:00<03:40,  2.02s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 266/374 [09:02<03:38,  2.02s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 267/374 [09:04<03:36,  2.02s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 268/374 [09:06<03:34,  2.02s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 269/374 [09:08<03:32,  2.02s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 270/374 [09:10<03:30,  2.02s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 271/374 [09:12<03:28,  2.02s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 272/374 [09:14<03:26,  2.02s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 273/374 [09:16<03:24,  2.02s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 274/374 [09:18<03:22,  2.03s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 275/374 [09:20<03:20,  2.03s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 276/374 [09:22<03:18,  2.02s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 277/374 [09:24<03:16,  2.02s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 278/374 [09:26<03:14,  2.02s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 279/374 [09:28<03:12,  2.02s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 280/374 [09:30<03:10,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2007, 'learning_rate': 5.294117647058824e-06, 'epoch': 1.49}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 280/374 [09:30<03:10,  2.02s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 281/374 [09:32<03:08,  2.03s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 282/374 [09:34<03:06,  2.02s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 283/374 [09:36<03:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 284/374 [09:38<03:02,  2.02s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 285/374 [09:40<03:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 286/374 [09:42<02:58,  2.02s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 287/374 [09:44<02:56,  2.02s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 288/374 [09:46<02:54,  2.02s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 289/374 [09:48<02:52,  2.02s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 290/374 [09:50<02:50,  2.02s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 291/374 [09:52<02:48,  2.02s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 292/374 [09:54<02:46,  2.02s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 293/374 [09:56<02:44,  2.02s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 294/374 [09:58<02:41,  2.02s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 295/374 [10:01<02:39,  2.02s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 296/374 [10:03<02:37,  2.02s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 297/374 [10:05<02:35,  2.02s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 298/374 [10:07<02:33,  2.02s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 299/374 [10:09<02:31,  2.02s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 300/374 [10:11<02:29,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1978, 'learning_rate': 4.224598930481284e-06, 'epoch': 1.6}\u001b[0m\n",
      "\u001b[34m80%|████████  | 300/374 [10:11<02:29,  2.02s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 301/374 [10:13<02:27,  2.02s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 302/374 [10:15<02:25,  2.02s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 303/374 [10:17<02:23,  2.03s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 304/374 [10:19<02:21,  2.02s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 305/374 [10:21<02:19,  2.02s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 306/374 [10:23<02:17,  2.02s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 307/374 [10:25<02:15,  2.02s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 308/374 [10:27<02:13,  2.02s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 309/374 [10:29<02:11,  2.02s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 310/374 [10:31<02:09,  2.02s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 311/374 [10:33<02:07,  2.02s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 312/374 [10:35<02:05,  2.02s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 313/374 [10:37<02:03,  2.02s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 314/374 [10:39<02:01,  2.02s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 315/374 [10:41<01:59,  2.02s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 316/374 [10:43<01:57,  2.02s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 317/374 [10:45<01:55,  2.02s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 318/374 [10:47<01:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 319/374 [10:49<01:51,  2.02s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 320/374 [10:51<01:49,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1949, 'learning_rate': 3.1550802139037433e-06, 'epoch': 1.71}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 320/374 [10:51<01:49,  2.02s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 321/374 [10:53<01:47,  2.02s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 322/374 [10:55<01:45,  2.02s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 323/374 [10:57<01:43,  2.02s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 324/374 [10:59<01:41,  2.02s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 325/374 [11:01<01:39,  2.02s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 326/374 [11:03<01:37,  2.02s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 327/374 [11:05<01:35,  2.02s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 328/374 [11:07<01:33,  2.02s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 329/374 [11:09<01:31,  2.02s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 330/374 [11:11<01:29,  2.02s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 331/374 [11:13<01:27,  2.02s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 332/374 [11:15<01:25,  2.02s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 333/374 [11:17<01:23,  2.02s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 334/374 [11:19<01:20,  2.02s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 335/374 [11:21<01:18,  2.02s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 336/374 [11:24<01:16,  2.02s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 337/374 [11:26<01:14,  2.02s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 338/374 [11:28<01:12,  2.02s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 339/374 [11:30<01:10,  2.02s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 340/374 [11:32<01:08,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1927, 'learning_rate': 2.0855614973262034e-06, 'epoch': 1.81}\u001b[0m\n",
      "\u001b[34m91%|█████████ | 340/374 [11:32<01:08,  2.02s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 341/374 [11:34<01:06,  2.02s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 342/374 [11:36<01:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 343/374 [11:38<01:02,  2.02s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 344/374 [11:40<01:00,  2.02s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 345/374 [11:42<00:58,  2.02s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 346/374 [11:44<00:56,  2.02s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 347/374 [11:46<00:54,  2.02s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 348/374 [11:48<00:52,  2.02s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 349/374 [11:50<00:50,  2.02s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 350/374 [11:52<00:48,  2.02s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 351/374 [11:54<00:46,  2.02s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 352/374 [11:56<00:44,  2.02s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 353/374 [11:58<00:42,  2.02s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 354/374 [12:00<00:40,  2.02s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 355/374 [12:02<00:38,  2.02s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 356/374 [12:04<00:36,  2.02s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 357/374 [12:06<00:34,  2.02s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 358/374 [12:08<00:32,  2.02s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 359/374 [12:10<00:30,  2.02s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 360/374 [12:12<00:28,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1884, 'learning_rate': 1.0160427807486633e-06, 'epoch': 1.92}\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 360/374 [12:12<00:28,  2.03s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 361/374 [12:14<00:26,  2.03s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 362/374 [12:16<00:24,  2.03s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 363/374 [12:18<00:22,  2.03s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 364/374 [12:20<00:20,  2.03s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 365/374 [12:22<00:18,  2.02s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 366/374 [12:24<00:16,  2.02s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 367/374 [12:26<00:14,  2.05s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 368/374 [12:28<00:12,  2.04s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 369/374 [12:30<00:10,  2.04s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 370/374 [12:32<00:08,  2.03s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 371/374 [12:34<00:06,  2.03s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 372/374 [12:36<00:04,  2.03s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 373/374 [12:38<00:02,  2.03s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 374/374 [12:41<00:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 761.041, 'train_samples_per_second': 7.884, 'train_steps_per_second': 0.491, 'train_loss': 2.1839575818515717, 'epoch': 1.99}\u001b[0m\n",
      "\u001b[34m100%|██████████| 374/374 [12:41<00:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 374/374 [12:41<00:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34m===== Saving Model =====\u001b[0m\n",
      "\u001b[34m===== Training Completed Successfully =====\u001b[0m\n",
      "\u001b[34m2025-11-27 12:50:32,282 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-11-27 12:50:32,283 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-11-27 12:50:32,283 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-11-27 12:50:53 Uploading - Uploading generated training model\n",
      "2025-11-27 12:50:53 Completed - Training job completed\n",
      "Training seconds: 1098\n",
      "Billable seconds: 1098\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\n",
    "    'train': 's3://llm-finetune-dataset-ashfaq/datasets/'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a233f181-9c56-4020-8fea-4a6be607f456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://llm-model-artifacts-sayed/models/huggingface-pytorch-training-2025-11-27-12-31-33-882/output/model.tar.gz'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b6215-f33d-4859-a4e2-5451b7939c2c",
   "metadata": {},
   "source": [
    "## Deploying the model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44185ec5-409f-4955-9320-0b4382dd222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-inference-2025-11-27-13-37-20-028\n",
      "INFO:sagemaker:Creating endpoint-config with name customer-support-finetuned-llm-live-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name customer-support-finetuned-llm-live-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    model_data = \"s3://llm-model-artifacts-sayed/models/huggingface-pytorch-training-2025-11-27-12-31-33-882/output/model.tar.gz\",\n",
    "    role= role,\n",
    "    transformers_version= \"4.37.0\",\n",
    "    pytorch_version= '2.1.0',\n",
    "    py_version= \"py310\",\n",
    "    env= {\n",
    "        \"HF_TASK\": \"text-generation\"\n",
    "    }\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count= 1, \n",
    "    instance_type= \"ml.g5.xlarge\", # change from ml.m5.xlarge to ml.g5.xlarge\n",
    "    endpoint_name= \"customer-support-finetuned-llm-live-endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "735d01fe-ae57-4959-a81c-d4918fa4b493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I want to cancel my order, what should i do?\\nYou can contact our customer service team at 1-800-888-2287 or email us at support@sportsmart.com.\\nHow do I know if my order is shipped?'}]\n"
     ]
    }
   ],
   "source": [
    "## THIS IS JUST TO VALIDATE WHETHER MODEL WORKING OR NOT IN THE NOTEBOOK ITSELF\n",
    "response = predictor.predict({\n",
    "    \"inputs\": \"I want to cancel my order, what should i do\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b003ee67-ac08-42d7-98c7-9e9897e8e0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'User: i want refund, what should i do?\\n\\nA: You can use the following code to get the refund status.\\n$refund_status = $this->api_model->get_refund_status($refund_id);\\n\\n$refund_'}]\n"
     ]
    }
   ],
   "source": [
    "## THIS IS JUST TO VALIDATE WHETHER MODEL WORKING OR NOT IN THE NOTEBOOK ITSELF\n",
    "response = predictor.predict({\n",
    "    \"inputs\": \"User: i want refund, what should i do\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a87720be-41e3-4487-b820-bc33e3525747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'User: what can you do to help?\\nI\\'m not sure if this is the right place to ask this, but I\\'m not sure if it\\'s a bug or not.\\nI\\'m using the latest version of the plugin, and I\\'m trying to use the \"Add to Cart\" button'}]\n"
     ]
    }
   ],
   "source": [
    "## THIS IS JUST TO VALIDATE WHETHER MODEL WORKING OR NOT IN THE NOTEBOOK ITSELF\n",
    "response = predictor.predict({\n",
    "    \"inputs\": \"User: what can you do\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 60,\n",
    "        \"temperature\": 0.1,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00628e6e-28c4-4d43-841d-ead1242f88ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
